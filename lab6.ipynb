{"cells":[{"cell_type":"code","source":["%sql\nCreate database if not exists Sample"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e8585cd8-b72c-484e-a588-8ae563f9e009"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["spark.sql(\"Create database if not exists Sample\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a1448993-1ce0-44da-b7de-ac42b6bb9571"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[1]: DataFrame[]","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[1]: DataFrame[]"]}}],"execution_count":0},{"cell_type":"code","source":["%sql\n\nDROP TABLE IF EXISTS Sample.Transactions;\n\nCREATE TABLE IF NOT EXISTS Sample.Transactions ( AccountId INT, TranDate DATE, TranAmt DECIMAL(8, 2));\n\nCREATE TABLE IF NOT EXISTS Sample.Logical (RowID INT,FName VARCHAR(20), Salary SMALLINT);"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"75aef164-8519-4784-99ff-021b245ae230"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"com.databricks.backend.common.rpc.DatabricksExceptions$SQLExecutionException: org.apache.spark.sql.AnalysisException: Cannot create table ('`spark_catalog`.`Sample`.`Transactions`'). The associated location ('dbfs:/user/hive/warehouse/sample.db/transactions') is not empty but it's not a Delta table\n\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.assertPathEmpty(CreateDeltaTableCommand.scala:264)\n\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.createTransactionLogOrVerify$1(CreateDeltaTableCommand.scala:176)\n\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.$anonfun$run$2(CreateDeltaTableCommand.scala:201)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag(DeltaLogging.scala:156)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag$(DeltaLogging.scala:143)\n\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.withOperationTypeTag(CreateDeltaTableCommand.scala:54)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperation$6(DeltaLogging.scala:121)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:171)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:169)\n\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordFrameProfile(CreateDeltaTableCommand.scala:54)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperation$5(DeltaLogging.scala:120)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:330)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:424)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:444)\n\tat com.databricks.logging.Log4jUsageLoggingShim$.$anonfun$withAttributionContext$1(Log4jUsageLoggingShim.scala:28)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:94)\n\tat com.databricks.logging.Log4jUsageLoggingShim$.withAttributionContext(Log4jUsageLoggingShim.scala:26)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:205)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:204)\n\tat com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:20)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:240)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:225)\n\tat com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:20)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:419)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:339)\n\tat com.databricks.spark.util.PublicDBLogging.recordOperationWithResultTags(DatabricksSparkUsageLogger.scala:20)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:330)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:302)\n\tat com.databricks.spark.util.PublicDBLogging.recordOperation(DatabricksSparkUsageLogger.scala:20)\n\tat com.databricks.spark.util.PublicDBLogging.recordOperation0(DatabricksSparkUsageLogger.scala:57)\n\tat com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:137)\n\tat com.databricks.spark.util.UsageLogger.recordOperation(UsageLogger.scala:73)\n\tat com.databricks.spark.util.UsageLogger.recordOperation$(UsageLogger.scala:60)\n\tat com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:98)\n\tat com.databricks.spark.util.UsageLogging.recordOperation(UsageLogger.scala:431)\n\tat com.databricks.spark.util.UsageLogging.recordOperation$(UsageLogger.scala:410)\n\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordOperation(CreateDeltaTableCommand.scala:54)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:119)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:104)\n\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordDeltaOperation(CreateDeltaTableCommand.scala:54)\n\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.run(CreateDeltaTableCommand.scala:118)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.com$databricks$sql$transaction$tahoe$catalog$DeltaCatalog$$createDeltaTable(DeltaCatalog.scala:215)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.createTable(DeltaCatalog.scala:425)\n\tat com.databricks.sql.managedcatalog.UnityCatalogV2Proxy.createTable(UnityCatalogV2Proxy.scala:208)\n\tat org.apache.spark.sql.execution.datasources.v2.CreateTableExec.run(CreateTableExec.scala:45)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$1(QueryExecution.scala:144)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$6(SQLExecution.scala:195)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:342)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:153)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:956)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:115)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:292)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:144)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:140)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:485)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:87)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:485)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:268)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:264)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:461)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:140)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:324)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:140)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:125)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:116)\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:225)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:104)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:956)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:101)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:791)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:956)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:786)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:695)\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.$anonfun$executeSql$1(SQLDriverLocal.scala:91)\n\tat scala.collection.immutable.List.map(List.scala:297)\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.executeSql(SQLDriverLocal.scala:37)\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.repl(SQLDriverLocal.scala:145)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$11(DriverLocal.scala:602)\n\tat com.databricks.logging.Log4jUsageLoggingShim$.$anonfun$withAttributionContext$1(Log4jUsageLoggingShim.scala:28)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:94)\n\tat com.databricks.logging.Log4jUsageLoggingShim$.withAttributionContext(Log4jUsageLoggingShim.scala:26)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:205)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:204)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:60)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:240)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:225)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:60)\n\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:579)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:615)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:607)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommandAndGetError(DriverWrapper.scala:526)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:561)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:431)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:374)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:225)\n\tat java.lang.Thread.run(Thread.java:748)\n\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.executeSql(SQLDriverLocal.scala:130)\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.repl(SQLDriverLocal.scala:145)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$11(DriverLocal.scala:602)\n\tat com.databricks.logging.Log4jUsageLoggingShim$.$anonfun$withAttributionContext$1(Log4jUsageLoggingShim.scala:28)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:94)\n\tat com.databricks.logging.Log4jUsageLoggingShim$.withAttributionContext(Log4jUsageLoggingShim.scala:26)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:205)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:204)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:60)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:240)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:225)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:60)\n\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:579)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:615)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:607)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommandAndGetError(DriverWrapper.scala:526)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:561)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:431)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:374)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:225)\n\tat java.lang.Thread.run(Thread.java:748)\n","errorSummary":"Error in SQL statement: AnalysisException: Cannot create table ('`spark_catalog`.`Sample`.`Transactions`'). The associated location ('dbfs:/user/hive/warehouse/sample.db/transactions') is not empty but it's not a Delta table","metadata":{},"errorTraceType":"html","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\ncom.databricks.backend.common.rpc.DatabricksExceptions$SQLExecutionException: org.apache.spark.sql.AnalysisException: Cannot create table ('`spark_catalog`.`Sample`.`Transactions`'). The associated location ('dbfs:/user/hive/warehouse/sample.db/transactions') is not empty but it's not a Delta table\n\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.assertPathEmpty(CreateDeltaTableCommand.scala:264)\n\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.createTransactionLogOrVerify$1(CreateDeltaTableCommand.scala:176)\n\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.$anonfun$run$2(CreateDeltaTableCommand.scala:201)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag(DeltaLogging.scala:156)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag$(DeltaLogging.scala:143)\n\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.withOperationTypeTag(CreateDeltaTableCommand.scala:54)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperation$6(DeltaLogging.scala:121)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:171)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:169)\n\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordFrameProfile(CreateDeltaTableCommand.scala:54)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperation$5(DeltaLogging.scala:120)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:330)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:424)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:444)\n\tat com.databricks.logging.Log4jUsageLoggingShim$.$anonfun$withAttributionContext$1(Log4jUsageLoggingShim.scala:28)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:94)\n\tat com.databricks.logging.Log4jUsageLoggingShim$.withAttributionContext(Log4jUsageLoggingShim.scala:26)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:205)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:204)\n\tat com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:20)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:240)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:225)\n\tat com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:20)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:419)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:339)\n\tat com.databricks.spark.util.PublicDBLogging.recordOperationWithResultTags(DatabricksSparkUsageLogger.scala:20)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:330)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:302)\n\tat com.databricks.spark.util.PublicDBLogging.recordOperation(DatabricksSparkUsageLogger.scala:20)\n\tat com.databricks.spark.util.PublicDBLogging.recordOperation0(DatabricksSparkUsageLogger.scala:57)\n\tat com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:137)\n\tat com.databricks.spark.util.UsageLogger.recordOperation(UsageLogger.scala:73)\n\tat com.databricks.spark.util.UsageLogger.recordOperation$(UsageLogger.scala:60)\n\tat com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:98)\n\tat com.databricks.spark.util.UsageLogging.recordOperation(UsageLogger.scala:431)\n\tat com.databricks.spark.util.UsageLogging.recordOperation$(UsageLogger.scala:410)\n\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordOperation(CreateDeltaTableCommand.scala:54)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:119)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:104)\n\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordDeltaOperation(CreateDeltaTableCommand.scala:54)\n\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.run(CreateDeltaTableCommand.scala:118)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.com$databricks$sql$transaction$tahoe$catalog$DeltaCatalog$$createDeltaTable(DeltaCatalog.scala:215)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.createTable(DeltaCatalog.scala:425)\n\tat com.databricks.sql.managedcatalog.UnityCatalogV2Proxy.createTable(UnityCatalogV2Proxy.scala:208)\n\tat org.apache.spark.sql.execution.datasources.v2.CreateTableExec.run(CreateTableExec.scala:45)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$1(QueryExecution.scala:144)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$6(SQLExecution.scala:195)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:342)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:153)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:956)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:115)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:292)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:144)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:140)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:485)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:87)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:485)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:268)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:264)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:461)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:140)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:324)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:140)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:125)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:116)\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:225)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:104)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:956)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:101)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:791)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:956)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:786)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:695)\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.$anonfun$executeSql$1(SQLDriverLocal.scala:91)\n\tat scala.collection.immutable.List.map(List.scala:297)\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.executeSql(SQLDriverLocal.scala:37)\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.repl(SQLDriverLocal.scala:145)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$11(DriverLocal.scala:602)\n\tat com.databricks.logging.Log4jUsageLoggingShim$.$anonfun$withAttributionContext$1(Log4jUsageLoggingShim.scala:28)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:94)\n\tat com.databricks.logging.Log4jUsageLoggingShim$.withAttributionContext(Log4jUsageLoggingShim.scala:26)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:205)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:204)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:60)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:240)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:225)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:60)\n\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:579)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:615)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:607)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommandAndGetError(DriverWrapper.scala:526)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:561)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:431)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:374)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:225)\n\tat java.lang.Thread.run(Thread.java:748)\n\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.executeSql(SQLDriverLocal.scala:130)\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.repl(SQLDriverLocal.scala:145)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$11(DriverLocal.scala:602)\n\tat com.databricks.logging.Log4jUsageLoggingShim$.$anonfun$withAttributionContext$1(Log4jUsageLoggingShim.scala:28)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:94)\n\tat com.databricks.logging.Log4jUsageLoggingShim$.withAttributionContext(Log4jUsageLoggingShim.scala:26)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:205)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:204)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:60)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:240)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:225)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:60)\n\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:579)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:615)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:607)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommandAndGetError(DriverWrapper.scala:526)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:561)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:431)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:374)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:225)\n\tat java.lang.Thread.run(Thread.java:748)"]}}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import sequence\ncolumns_transactions = sequence(\"AccountId\",\"TranDate\", \"TranAmt\")\n\ncolumns_logical = sequence(\"RowID\",\"FName\", \"Salary\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"21e6a184-133d-4e26-9e1b-f214f586805c"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql\n\nINSERT INTO Sample.Transactions VALUES \n( 1, '2011-01-01', 500),\n( 1, '2011-01-15', 50),\n( 1, '2011-01-22', 250),\n( 1, '2011-01-24', 75),\n( 1, '2011-01-26', 125),\n( 1, '2011-01-28', 175),\n( 2, '2011-01-01', 500),\n( 2, '2011-01-15', 50),\n( 2, '2011-01-22', 25),\n( 2, '2011-01-23', 125),\n( 2, '2011-01-26', 200),\n( 2, '2011-01-29', 250),\n( 3, '2011-01-01', 500),\n( 3, '2011-01-15', 50 ),\n( 3, '2011-01-22', 5000),\n( 3, '2011-01-25', 550),\n( 3, '2011-01-27', 95 ),\n( 3, '2011-01-30', 2500)\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fc34651d-c76a-4b46-992f-f95400ffa3f3"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"com.databricks.backend.common.rpc.DatabricksExceptions$SQLExecutionException: org.apache.spark.sql.AnalysisException: Table not found: Sample.Transactions; line 1 pos 12;\n'InsertIntoStatement 'UnresolvedRelation [Sample, Transactions], [], false, false, false\n+- LocalRelation [col1#81, col2#82, col3#83]\n\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$2(CheckAnalysis.scala:151)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$2$adapted(CheckAnalysis.scala:104)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:268)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:104)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:99)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:99)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:222)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:276)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:331)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:273)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:112)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:151)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:246)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:956)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:246)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:113)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:110)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:102)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:103)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:956)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:101)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:791)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:956)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:786)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:695)\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.$anonfun$executeSql$1(SQLDriverLocal.scala:91)\n\tat scala.collection.immutable.List.map(List.scala:293)\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.executeSql(SQLDriverLocal.scala:37)\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.repl(SQLDriverLocal.scala:145)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$11(DriverLocal.scala:602)\n\tat com.databricks.logging.Log4jUsageLoggingShim$.$anonfun$withAttributionContext$1(Log4jUsageLoggingShim.scala:28)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:94)\n\tat com.databricks.logging.Log4jUsageLoggingShim$.withAttributionContext(Log4jUsageLoggingShim.scala:26)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:205)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:204)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:60)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:240)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:225)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:60)\n\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:579)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:615)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:607)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommandAndGetError(DriverWrapper.scala:526)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:561)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:431)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:374)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:225)\n\tat java.lang.Thread.run(Thread.java:748)\n\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.executeSql(SQLDriverLocal.scala:130)\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.repl(SQLDriverLocal.scala:145)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$11(DriverLocal.scala:602)\n\tat com.databricks.logging.Log4jUsageLoggingShim$.$anonfun$withAttributionContext$1(Log4jUsageLoggingShim.scala:28)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:94)\n\tat com.databricks.logging.Log4jUsageLoggingShim$.withAttributionContext(Log4jUsageLoggingShim.scala:26)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:205)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:204)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:60)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:240)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:225)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:60)\n\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:579)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:615)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:607)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommandAndGetError(DriverWrapper.scala:526)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:561)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:431)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:374)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:225)\n\tat java.lang.Thread.run(Thread.java:748)\n","errorSummary":"Error in SQL statement: AnalysisException: Table not found: Sample.Transactions; line 1 pos 12;\n'InsertIntoStatement 'UnresolvedRelation [Sample, Transactions], [], false, false, false\n+- LocalRelation [col1#81, col2#82, col3#83]\n","metadata":{},"errorTraceType":"html","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\ncom.databricks.backend.common.rpc.DatabricksExceptions$SQLExecutionException: org.apache.spark.sql.AnalysisException: Table not found: Sample.Transactions; line 1 pos 12;\n'InsertIntoStatement 'UnresolvedRelation [Sample, Transactions], [], false, false, false\n+- LocalRelation [col1#81, col2#82, col3#83]\n\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$2(CheckAnalysis.scala:151)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$2$adapted(CheckAnalysis.scala:104)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:268)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:104)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:99)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:99)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:222)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:276)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:331)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:273)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:112)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:151)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:246)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:956)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:246)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:113)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:110)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:102)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:103)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:956)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:101)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:791)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:956)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:786)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:695)\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.$anonfun$executeSql$1(SQLDriverLocal.scala:91)\n\tat scala.collection.immutable.List.map(List.scala:293)\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.executeSql(SQLDriverLocal.scala:37)\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.repl(SQLDriverLocal.scala:145)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$11(DriverLocal.scala:602)\n\tat com.databricks.logging.Log4jUsageLoggingShim$.$anonfun$withAttributionContext$1(Log4jUsageLoggingShim.scala:28)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:94)\n\tat com.databricks.logging.Log4jUsageLoggingShim$.withAttributionContext(Log4jUsageLoggingShim.scala:26)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:205)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:204)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:60)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:240)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:225)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:60)\n\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:579)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:615)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:607)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommandAndGetError(DriverWrapper.scala:526)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:561)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:431)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:374)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:225)\n\tat java.lang.Thread.run(Thread.java:748)\n\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.executeSql(SQLDriverLocal.scala:130)\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.repl(SQLDriverLocal.scala:145)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$11(DriverLocal.scala:602)\n\tat com.databricks.logging.Log4jUsageLoggingShim$.$anonfun$withAttributionContext$1(Log4jUsageLoggingShim.scala:28)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:94)\n\tat com.databricks.logging.Log4jUsageLoggingShim$.withAttributionContext(Log4jUsageLoggingShim.scala:26)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:205)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:204)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:60)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:240)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:225)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:60)\n\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:579)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:615)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:607)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommandAndGetError(DriverWrapper.scala:526)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:561)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:431)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:374)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:225)\n\tat java.lang.Thread.run(Thread.java:748)"]}}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql import Row\nfrom pyspark.sql.functions import to_date,col\n\ndata_transactions = spark.createDataFrame([( 1, \"2011-01-01\", 500), \\\n( 1, \"2011-01-15\", 50), \\\n( 1, \"2011-01-22\", 250), \\\n( 1, \"2011-01-24\", 75), \\\n( 1, \"2011-01-26\", 125), \\\n( 1, \"2011-01-28\", 175), \\\n( 2, \"2011-01-01\", 500), \\\n( 2, \"2011-01-15\", 50), \\\n( 2, \"2011-01-22\", 25), \\\n( 2, \"2011-01-23\", 125), \\\n( 2, \"2011-01-26\", 200), \\\n( 2, \"2011-01-29\", 250), \\\n( 3, \"2011-01-01\", 500), \\\n( 3, \"2011-01-15\", 50 ), \\\n( 3, \"2011-01-22\", 5000), \\\n( 3, \"2011-01-25\", 550), \\\n( 3, \"2011-01-27\", 95 ), \\\n( 3, \"2011-01-30\", 2500)], ['AccountId', 'TranDate', 'TranAmt']).withColumn(\"TranDate\", to_date(col(\"TranDate\")))\n\ndata_transactions.write.insertInto('Sample.Transactions')\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b5f8092c-b1fc-4d34-b99b-c5396123c446"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n\u001B[0;32m<command-126973269320614>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     21\u001B[0m ( 3, \"2011-01-30\", 2500)], ['AccountId', 'TranDate', 'TranAmt']).withColumn(\"TranDate\", to_date(col(\"TranDate\")))\n\u001B[1;32m     22\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 23\u001B[0;31m \u001B[0mdata_transactions\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwrite\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0minsertInto\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'Sample.Transactions'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\n\u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py\u001B[0m in \u001B[0;36minsertInto\u001B[0;34m(self, tableName, overwrite)\u001B[0m\n\u001B[1;32m    760\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0moverwrite\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    761\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmode\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"overwrite\"\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0moverwrite\u001B[0m \u001B[0;32melse\u001B[0m \u001B[0;34m\"append\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 762\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jwrite\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0minsertInto\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtableName\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    763\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    764\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0msaveAsTable\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mname\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mformat\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmode\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpartitionBy\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0moptions\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.1-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1302\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1303\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1304\u001B[0;31m         return_value = get_return_value(\n\u001B[0m\u001B[1;32m   1305\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[1;32m   1306\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    121\u001B[0m                 \u001B[0;31m# Hide where the exception came from that shows a non-Pythonic\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    122\u001B[0m                 \u001B[0;31m# JVM exception message.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 123\u001B[0;31m                 \u001B[0;32mraise\u001B[0m \u001B[0mconverted\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    124\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    125\u001B[0m                 \u001B[0;32mraise\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mAnalysisException\u001B[0m: Table or view 'transactions' not found in database 'sample'","errorSummary":"<span class='ansi-red-fg'>AnalysisException</span>: Table or view 'transactions' not found in database 'sample'","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n\u001B[0;32m<command-126973269320614>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     21\u001B[0m ( 3, \"2011-01-30\", 2500)], ['AccountId', 'TranDate', 'TranAmt']).withColumn(\"TranDate\", to_date(col(\"TranDate\")))\n\u001B[1;32m     22\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 23\u001B[0;31m \u001B[0mdata_transactions\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwrite\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0minsertInto\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'Sample.Transactions'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\n\u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py\u001B[0m in \u001B[0;36minsertInto\u001B[0;34m(self, tableName, overwrite)\u001B[0m\n\u001B[1;32m    760\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0moverwrite\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    761\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmode\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"overwrite\"\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0moverwrite\u001B[0m \u001B[0;32melse\u001B[0m \u001B[0;34m\"append\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 762\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jwrite\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0minsertInto\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtableName\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    763\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    764\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0msaveAsTable\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mname\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mformat\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmode\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpartitionBy\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0moptions\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.1-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1302\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1303\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1304\u001B[0;31m         return_value = get_return_value(\n\u001B[0m\u001B[1;32m   1305\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[1;32m   1306\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    121\u001B[0m                 \u001B[0;31m# Hide where the exception came from that shows a non-Pythonic\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    122\u001B[0m                 \u001B[0;31m# JVM exception message.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 123\u001B[0;31m                 \u001B[0;32mraise\u001B[0m \u001B[0mconverted\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    124\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    125\u001B[0m                 \u001B[0;32mraise\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mAnalysisException\u001B[0m: Table or view 'transactions' not found in database 'sample'"]}}],"execution_count":0},{"cell_type":"code","source":["%sql\nINSERT INTO Sample.Logical\nVALUES (1,'George', 800),\n(2,'Sam', 950),\n(3,'Diane', 1100),\n(4,'Nicholas', 1250),\n(5,'Samuel', 1250),\n(6,'Patricia', 1300),\n(7,'Brian', 1500),\n(8,'Thomas', 1600),\n(9,'Fran', 2450),\n(10,'Debbie', 2850),\n(11,'Mark', 2975),\n(12,'James', 3000),\n(13,'Cynthia', 3000),\n(14,'Christopher', 5000);"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b917c13f-da99-4910-acfc-d70b9a5e6c62"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[[14,14]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"num_affected_rows","type":"\"long\"","metadata":"{}"},{"name":"num_inserted_rows","type":"\"long\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>num_affected_rows</th><th>num_inserted_rows</th></tr></thead><tbody><tr><td>14</td><td>14</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["data_logical = spark.createDataFrame([\n(1,\"George\", 800),\n(2,\"Sam\", 950),\n(3,\"Diane\", 1100),\n(4,\"Nicholas\", 1250),\n(5,\"Samuel\", 1250),\n(6,\"Patricia\", 1300),\n(7,\"Brian\", 1500),\n(8,\"Thomas\", 1600),\n(9,\"Fran\", 2450),\n(10,\"Debbie\", 2850),\n(11,\"Mark\", 2975),\n(12,\"James\", 3000),\n(13,\"Cynthia\", 3000),\n(14,\"Christopher\", 5000)], [\"RowID\",\"FName\", \"Salary\"])\n\ndata_logical.write.insertInto('Sample.Logical')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"30a9bd05-6cda-473b-9f25-01923eb1f172"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n\u001B[0;32m<command-126973269320616>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     15\u001B[0m (14,\"Christopher\", 5000)], [\"RowID\",\"FName\", \"Salary\"])\n\u001B[1;32m     16\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 17\u001B[0;31m \u001B[0mdata_logical\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwrite\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0minsertInto\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'Sample.Logical'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\n\u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py\u001B[0m in \u001B[0;36minsertInto\u001B[0;34m(self, tableName, overwrite)\u001B[0m\n\u001B[1;32m    760\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0moverwrite\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    761\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmode\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"overwrite\"\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0moverwrite\u001B[0m \u001B[0;32melse\u001B[0m \u001B[0;34m\"append\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 762\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jwrite\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0minsertInto\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtableName\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    763\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    764\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0msaveAsTable\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mname\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mformat\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmode\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpartitionBy\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0moptions\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.1-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1302\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1303\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1304\u001B[0;31m         return_value = get_return_value(\n\u001B[0m\u001B[1;32m   1305\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[1;32m   1306\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    121\u001B[0m                 \u001B[0;31m# Hide where the exception came from that shows a non-Pythonic\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    122\u001B[0m                 \u001B[0;31m# JVM exception message.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 123\u001B[0;31m                 \u001B[0;32mraise\u001B[0m \u001B[0mconverted\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    124\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    125\u001B[0m                 \u001B[0;32mraise\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mAnalysisException\u001B[0m: Table or view 'logical' not found in database 'sample'","errorSummary":"<span class='ansi-red-fg'>AnalysisException</span>: Table or view 'logical' not found in database 'sample'","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n\u001B[0;32m<command-126973269320616>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     15\u001B[0m (14,\"Christopher\", 5000)], [\"RowID\",\"FName\", \"Salary\"])\n\u001B[1;32m     16\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 17\u001B[0;31m \u001B[0mdata_logical\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwrite\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0minsertInto\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'Sample.Logical'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\n\u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py\u001B[0m in \u001B[0;36minsertInto\u001B[0;34m(self, tableName, overwrite)\u001B[0m\n\u001B[1;32m    760\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0moverwrite\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    761\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmode\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"overwrite\"\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0moverwrite\u001B[0m \u001B[0;32melse\u001B[0m \u001B[0;34m\"append\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 762\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jwrite\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0minsertInto\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtableName\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    763\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    764\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0msaveAsTable\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mname\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mformat\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmode\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpartitionBy\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0moptions\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.1-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1302\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1303\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1304\u001B[0;31m         return_value = get_return_value(\n\u001B[0m\u001B[1;32m   1305\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[1;32m   1306\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    121\u001B[0m                 \u001B[0;31m# Hide where the exception came from that shows a non-Pythonic\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    122\u001B[0m                 \u001B[0;31m# JVM exception message.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 123\u001B[0;31m                 \u001B[0;32mraise\u001B[0m \u001B[0mconverted\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    124\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    125\u001B[0m                 \u001B[0;32mraise\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mAnalysisException\u001B[0m: Table or view 'logical' not found in database 'sample'"]}}],"execution_count":0},{"cell_type":"markdown","source":["Totals based on previous row"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e6c28f5d-d7c0-48d2-828f-313602b8183a"}}},{"cell_type":"code","source":["%sql\n\nSELECT AccountId,\nTranDate,\nTranAmt,\n-- running total of all transactions\nSUM(TranAmt) OVER (PARTITION BY AccountId ORDER BY TranDate) as RunTotalAmt\nFROM Sample.Transactions ORDER BY AccountId, TranDate;"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f17c10d3-d673-490a-81e3-072d0b6a0170"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[[1,"2011-01-01","500.00","500.00"],[1,"2011-01-15","50.00","550.00"],[1,"2011-01-22","250.00","800.00"],[1,"2011-01-24","75.00","875.00"],[1,"2011-01-26","125.00","1000.00"],[1,"2011-01-28","175.00","1175.00"],[2,"2011-01-01","500.00","500.00"],[2,"2011-01-15","50.00","550.00"],[2,"2011-01-22","25.00","575.00"],[2,"2011-01-23","125.00","700.00"],[2,"2011-01-26","200.00","900.00"],[2,"2011-01-29","250.00","1150.00"],[3,"2011-01-01","500.00","500.00"],[3,"2011-01-15","50.00","550.00"],[3,"2011-01-22","5000.00","5550.00"],[3,"2011-01-25","550.00","6100.00"],[3,"2011-01-27","95.00","6195.00"],[3,"2011-01-30","2500.00","8695.00"]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"AccountId","type":"\"integer\"","metadata":"{}"},{"name":"TranDate","type":"\"date\"","metadata":"{}"},{"name":"TranAmt","type":"\"decimal(8,2)\"","metadata":"{}"},{"name":"RunTotalAmt","type":"\"decimal(18,2)\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>AccountId</th><th>TranDate</th><th>TranAmt</th><th>RunTotalAmt</th></tr></thead><tbody><tr><td>1</td><td>2011-01-01</td><td>500.00</td><td>500.00</td></tr><tr><td>1</td><td>2011-01-15</td><td>50.00</td><td>550.00</td></tr><tr><td>1</td><td>2011-01-22</td><td>250.00</td><td>800.00</td></tr><tr><td>1</td><td>2011-01-24</td><td>75.00</td><td>875.00</td></tr><tr><td>1</td><td>2011-01-26</td><td>125.00</td><td>1000.00</td></tr><tr><td>1</td><td>2011-01-28</td><td>175.00</td><td>1175.00</td></tr><tr><td>2</td><td>2011-01-01</td><td>500.00</td><td>500.00</td></tr><tr><td>2</td><td>2011-01-15</td><td>50.00</td><td>550.00</td></tr><tr><td>2</td><td>2011-01-22</td><td>25.00</td><td>575.00</td></tr><tr><td>2</td><td>2011-01-23</td><td>125.00</td><td>700.00</td></tr><tr><td>2</td><td>2011-01-26</td><td>200.00</td><td>900.00</td></tr><tr><td>2</td><td>2011-01-29</td><td>250.00</td><td>1150.00</td></tr><tr><td>3</td><td>2011-01-01</td><td>500.00</td><td>500.00</td></tr><tr><td>3</td><td>2011-01-15</td><td>50.00</td><td>550.00</td></tr><tr><td>3</td><td>2011-01-22</td><td>5000.00</td><td>5550.00</td></tr><tr><td>3</td><td>2011-01-25</td><td>550.00</td><td>6100.00</td></tr><tr><td>3</td><td>2011-01-27</td><td>95.00</td><td>6195.00</td></tr><tr><td>3</td><td>2011-01-30</td><td>2500.00</td><td>8695.00</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fbb53d1c-4071-4a82-8346-13ad68ca5fa5"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[7]: DataFrame[_1: bigint, _2: string, _3: bigint]","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[7]: DataFrame[_1: bigint, _2: string, _3: bigint]"]}}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import sum\nfrom pyspark.sql.window import Window\n\nwindowSpec = Window.partitionBy(\"AccountId\").orderBy(\"TranDate\")\nsumTransactionsDf = data_transactions. \\\nwithColumn(\"RunTotalAmt\",\n           sum(\"TranAmt\"). \\\n           over(windowSpec)). \\\norderBy(\"AccountId\", \"TranDate\")\ndisplay(sumTransactionsDf)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"52e3c235-7e08-4edb-9e03-13decb3f7523"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"DataFrame[AccountId: bigint, TranDate: date, TranAmt: bigint]\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["DataFrame[AccountId: bigint, TranDate: date, TranAmt: bigint]\n"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[[1,"2011-01-01",500,500],[1,"2011-01-15",50,550],[1,"2011-01-22",250,800],[1,"2011-01-24",75,875],[1,"2011-01-26",125,1000],[1,"2011-01-28",175,1175],[2,"2011-01-01",500,500],[2,"2011-01-15",50,550],[2,"2011-01-22",25,575],[2,"2011-01-23",125,700],[2,"2011-01-26",200,900],[2,"2011-01-29",250,1150],[3,"2011-01-01",500,500],[3,"2011-01-15",50,550],[3,"2011-01-22",5000,5550],[3,"2011-01-25",550,6100],[3,"2011-01-27",95,6195],[3,"2011-01-30",2500,8695]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"AccountId","type":"\"long\"","metadata":"{}"},{"name":"TranDate","type":"\"date\"","metadata":"{}"},{"name":"TranAmt","type":"\"long\"","metadata":"{}"},{"name":"RunTotalAmt","type":"\"long\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>AccountId</th><th>TranDate</th><th>TranAmt</th><th>RunTotalAmt</th></tr></thead><tbody><tr><td>1</td><td>2011-01-01</td><td>500</td><td>500</td></tr><tr><td>1</td><td>2011-01-15</td><td>50</td><td>550</td></tr><tr><td>1</td><td>2011-01-22</td><td>250</td><td>800</td></tr><tr><td>1</td><td>2011-01-24</td><td>75</td><td>875</td></tr><tr><td>1</td><td>2011-01-26</td><td>125</td><td>1000</td></tr><tr><td>1</td><td>2011-01-28</td><td>175</td><td>1175</td></tr><tr><td>2</td><td>2011-01-01</td><td>500</td><td>500</td></tr><tr><td>2</td><td>2011-01-15</td><td>50</td><td>550</td></tr><tr><td>2</td><td>2011-01-22</td><td>25</td><td>575</td></tr><tr><td>2</td><td>2011-01-23</td><td>125</td><td>700</td></tr><tr><td>2</td><td>2011-01-26</td><td>200</td><td>900</td></tr><tr><td>2</td><td>2011-01-29</td><td>250</td><td>1150</td></tr><tr><td>3</td><td>2011-01-01</td><td>500</td><td>500</td></tr><tr><td>3</td><td>2011-01-15</td><td>50</td><td>550</td></tr><tr><td>3</td><td>2011-01-22</td><td>5000</td><td>5550</td></tr><tr><td>3</td><td>2011-01-25</td><td>550</td><td>6100</td></tr><tr><td>3</td><td>2011-01-27</td><td>95</td><td>6195</td></tr><tr><td>3</td><td>2011-01-30</td><td>2500</td><td>8695</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["%sql\nSELECT AccountId,\nTranDate,\nTranAmt,\n-- running average of all transactions\nAVG(TranAmt) OVER (PARTITION BY AccountId ORDER BY TranDate) as RunAvg,\n-- running total # of transactions\nCOUNT(*) OVER (PARTITION BY AccountId ORDER BY TranDate) as RunTranQty,\n-- smallest of the transactions so far\nMIN(TranAmt) OVER (PARTITION BY AccountId ORDER BY TranDate) as RunSmallAmt,\n-- largest of the transactions so far\nMAX(TranAmt) OVER (PARTITION BY AccountId ORDER BY TranDate) as RunLargeAmt,\n-- running total of all transactions\nSUM(TranAmt) OVER (PARTITION BY AccountId ORDER BY TranDate) RunTotalAmt\nFROM Sample.Transactions \nORDER BY AccountId,TranDate;"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1050028e-a28e-433b-bef8-865d842dfb5a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[[1,"2011-01-01","500.00","500.000000",1,"500.00","500.00","500.00"],[1,"2011-01-15","50.00","275.000000",2,"50.00","500.00","550.00"],[1,"2011-01-22","250.00","266.666667",3,"50.00","500.00","800.00"],[1,"2011-01-24","75.00","218.750000",4,"50.00","500.00","875.00"],[1,"2011-01-26","125.00","200.000000",5,"50.00","500.00","1000.00"],[1,"2011-01-28","175.00","195.833333",6,"50.00","500.00","1175.00"],[2,"2011-01-01","500.00","500.000000",1,"500.00","500.00","500.00"],[2,"2011-01-15","50.00","275.000000",2,"50.00","500.00","550.00"],[2,"2011-01-22","25.00","191.666667",3,"25.00","500.00","575.00"],[2,"2011-01-23","125.00","175.000000",4,"25.00","500.00","700.00"],[2,"2011-01-26","200.00","180.000000",5,"25.00","500.00","900.00"],[2,"2011-01-29","250.00","191.666667",6,"25.00","500.00","1150.00"],[3,"2011-01-01","500.00","500.000000",1,"500.00","500.00","500.00"],[3,"2011-01-15","50.00","275.000000",2,"50.00","500.00","550.00"],[3,"2011-01-22","5000.00","1850.000000",3,"50.00","5000.00","5550.00"],[3,"2011-01-25","550.00","1525.000000",4,"50.00","5000.00","6100.00"],[3,"2011-01-27","95.00","1239.000000",5,"50.00","5000.00","6195.00"],[3,"2011-01-30","2500.00","1449.166667",6,"50.00","5000.00","8695.00"]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"AccountId","type":"\"integer\"","metadata":"{}"},{"name":"TranDate","type":"\"date\"","metadata":"{}"},{"name":"TranAmt","type":"\"decimal(8,2)\"","metadata":"{}"},{"name":"RunAvg","type":"\"decimal(12,6)\"","metadata":"{}"},{"name":"RunTranQty","type":"\"long\"","metadata":"{}"},{"name":"RunSmallAmt","type":"\"decimal(8,2)\"","metadata":"{}"},{"name":"RunLargeAmt","type":"\"decimal(8,2)\"","metadata":"{}"},{"name":"RunTotalAmt","type":"\"decimal(18,2)\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>AccountId</th><th>TranDate</th><th>TranAmt</th><th>RunAvg</th><th>RunTranQty</th><th>RunSmallAmt</th><th>RunLargeAmt</th><th>RunTotalAmt</th></tr></thead><tbody><tr><td>1</td><td>2011-01-01</td><td>500.00</td><td>500.000000</td><td>1</td><td>500.00</td><td>500.00</td><td>500.00</td></tr><tr><td>1</td><td>2011-01-15</td><td>50.00</td><td>275.000000</td><td>2</td><td>50.00</td><td>500.00</td><td>550.00</td></tr><tr><td>1</td><td>2011-01-22</td><td>250.00</td><td>266.666667</td><td>3</td><td>50.00</td><td>500.00</td><td>800.00</td></tr><tr><td>1</td><td>2011-01-24</td><td>75.00</td><td>218.750000</td><td>4</td><td>50.00</td><td>500.00</td><td>875.00</td></tr><tr><td>1</td><td>2011-01-26</td><td>125.00</td><td>200.000000</td><td>5</td><td>50.00</td><td>500.00</td><td>1000.00</td></tr><tr><td>1</td><td>2011-01-28</td><td>175.00</td><td>195.833333</td><td>6</td><td>50.00</td><td>500.00</td><td>1175.00</td></tr><tr><td>2</td><td>2011-01-01</td><td>500.00</td><td>500.000000</td><td>1</td><td>500.00</td><td>500.00</td><td>500.00</td></tr><tr><td>2</td><td>2011-01-15</td><td>50.00</td><td>275.000000</td><td>2</td><td>50.00</td><td>500.00</td><td>550.00</td></tr><tr><td>2</td><td>2011-01-22</td><td>25.00</td><td>191.666667</td><td>3</td><td>25.00</td><td>500.00</td><td>575.00</td></tr><tr><td>2</td><td>2011-01-23</td><td>125.00</td><td>175.000000</td><td>4</td><td>25.00</td><td>500.00</td><td>700.00</td></tr><tr><td>2</td><td>2011-01-26</td><td>200.00</td><td>180.000000</td><td>5</td><td>25.00</td><td>500.00</td><td>900.00</td></tr><tr><td>2</td><td>2011-01-29</td><td>250.00</td><td>191.666667</td><td>6</td><td>25.00</td><td>500.00</td><td>1150.00</td></tr><tr><td>3</td><td>2011-01-01</td><td>500.00</td><td>500.000000</td><td>1</td><td>500.00</td><td>500.00</td><td>500.00</td></tr><tr><td>3</td><td>2011-01-15</td><td>50.00</td><td>275.000000</td><td>2</td><td>50.00</td><td>500.00</td><td>550.00</td></tr><tr><td>3</td><td>2011-01-22</td><td>5000.00</td><td>1850.000000</td><td>3</td><td>50.00</td><td>5000.00</td><td>5550.00</td></tr><tr><td>3</td><td>2011-01-25</td><td>550.00</td><td>1525.000000</td><td>4</td><td>50.00</td><td>5000.00</td><td>6100.00</td></tr><tr><td>3</td><td>2011-01-27</td><td>95.00</td><td>1239.000000</td><td>5</td><td>50.00</td><td>5000.00</td><td>6195.00</td></tr><tr><td>3</td><td>2011-01-30</td><td>2500.00</td><td>1449.166667</td><td>6</td><td>50.00</td><td>5000.00</td><td>8695.00</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["\nfrom pyspark.sql.functions import avg,count,min,max\n\nwindowSpec = Window.partitionBy(\"AccountId\").orderBy(\"TranDate\")\nsumTransactionsDf = data_transactions \\\n  .withColumn(\"RunAvg\",avg(\"TranAmt\").over(windowSpec)) \\\n  .withColumn(\"RunTranQty\",count(\"*\").over(windowSpec)) \\\n  .withColumn(\"RunSmallAmt\",min(\"TranAmt\").over(windowSpec)) \\\n  .withColumn(\"RunLargeAmt\",max(\"TranAmt\").over(windowSpec)) \\\n  .withColumn(\"RunTotalAmt\",sum(\"TranAmt\").over(windowSpec)) \\\n  .orderBy(\"AccountId\", \"TranDate\")\ndisplay(sumTransactionsDf)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d2dd6cc2-7961-48dc-b5a9-1dfd2b59eeff"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[[1,"2011-01-01",500,500.0,1,500,500,500],[1,"2011-01-15",50,275.0,2,50,500,550],[1,"2011-01-22",250,266.6666666666667,3,50,500,800],[1,"2011-01-24",75,218.75,4,50,500,875],[1,"2011-01-26",125,200.0,5,50,500,1000],[1,"2011-01-28",175,195.83333333333334,6,50,500,1175],[2,"2011-01-01",500,500.0,1,500,500,500],[2,"2011-01-15",50,275.0,2,50,500,550],[2,"2011-01-22",25,191.66666666666666,3,25,500,575],[2,"2011-01-23",125,175.0,4,25,500,700],[2,"2011-01-26",200,180.0,5,25,500,900],[2,"2011-01-29",250,191.66666666666666,6,25,500,1150],[3,"2011-01-01",500,500.0,1,500,500,500],[3,"2011-01-15",50,275.0,2,50,500,550],[3,"2011-01-22",5000,1850.0,3,50,5000,5550],[3,"2011-01-25",550,1525.0,4,50,5000,6100],[3,"2011-01-27",95,1239.0,5,50,5000,6195],[3,"2011-01-30",2500,1449.1666666666667,6,50,5000,8695]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"AccountId","type":"\"long\"","metadata":"{}"},{"name":"TranDate","type":"\"date\"","metadata":"{}"},{"name":"TranAmt","type":"\"long\"","metadata":"{}"},{"name":"RunAvg","type":"\"double\"","metadata":"{}"},{"name":"RunTranQty","type":"\"long\"","metadata":"{}"},{"name":"RunSmallAmt","type":"\"long\"","metadata":"{}"},{"name":"RunLargeAmt","type":"\"long\"","metadata":"{}"},{"name":"RunTotalAmt","type":"\"long\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>AccountId</th><th>TranDate</th><th>TranAmt</th><th>RunAvg</th><th>RunTranQty</th><th>RunSmallAmt</th><th>RunLargeAmt</th><th>RunTotalAmt</th></tr></thead><tbody><tr><td>1</td><td>2011-01-01</td><td>500</td><td>500.0</td><td>1</td><td>500</td><td>500</td><td>500</td></tr><tr><td>1</td><td>2011-01-15</td><td>50</td><td>275.0</td><td>2</td><td>50</td><td>500</td><td>550</td></tr><tr><td>1</td><td>2011-01-22</td><td>250</td><td>266.6666666666667</td><td>3</td><td>50</td><td>500</td><td>800</td></tr><tr><td>1</td><td>2011-01-24</td><td>75</td><td>218.75</td><td>4</td><td>50</td><td>500</td><td>875</td></tr><tr><td>1</td><td>2011-01-26</td><td>125</td><td>200.0</td><td>5</td><td>50</td><td>500</td><td>1000</td></tr><tr><td>1</td><td>2011-01-28</td><td>175</td><td>195.83333333333334</td><td>6</td><td>50</td><td>500</td><td>1175</td></tr><tr><td>2</td><td>2011-01-01</td><td>500</td><td>500.0</td><td>1</td><td>500</td><td>500</td><td>500</td></tr><tr><td>2</td><td>2011-01-15</td><td>50</td><td>275.0</td><td>2</td><td>50</td><td>500</td><td>550</td></tr><tr><td>2</td><td>2011-01-22</td><td>25</td><td>191.66666666666666</td><td>3</td><td>25</td><td>500</td><td>575</td></tr><tr><td>2</td><td>2011-01-23</td><td>125</td><td>175.0</td><td>4</td><td>25</td><td>500</td><td>700</td></tr><tr><td>2</td><td>2011-01-26</td><td>200</td><td>180.0</td><td>5</td><td>25</td><td>500</td><td>900</td></tr><tr><td>2</td><td>2011-01-29</td><td>250</td><td>191.66666666666666</td><td>6</td><td>25</td><td>500</td><td>1150</td></tr><tr><td>3</td><td>2011-01-01</td><td>500</td><td>500.0</td><td>1</td><td>500</td><td>500</td><td>500</td></tr><tr><td>3</td><td>2011-01-15</td><td>50</td><td>275.0</td><td>2</td><td>50</td><td>500</td><td>550</td></tr><tr><td>3</td><td>2011-01-22</td><td>5000</td><td>1850.0</td><td>3</td><td>50</td><td>5000</td><td>5550</td></tr><tr><td>3</td><td>2011-01-25</td><td>550</td><td>1525.0</td><td>4</td><td>50</td><td>5000</td><td>6100</td></tr><tr><td>3</td><td>2011-01-27</td><td>95</td><td>1239.0</td><td>5</td><td>50</td><td>5000</td><td>6195</td></tr><tr><td>3</td><td>2011-01-30</td><td>2500</td><td>1449.1666666666667</td><td>6</td><td>50</td><td>5000</td><td>8695</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["%sql\nSELECT AccountId,\nTranDate,\nTranAmt,\n-- average of the current and previous 2 transactions\nAVG(TranAmt) OVER (PARTITION BY AccountId ORDER BY TranDate ROWS BETWEEN 2 PRECEDING AND CURRENT ROW) as SlideAvg,\n-- total # of the current and previous 2 transactions\nCOUNT(*) OVER (PARTITION BY AccountId ORDER BY TranDate ROWS BETWEEN 2 PRECEDING AND CURRENT ROW) as SlideQty,\n-- smallest of the current and previous 2 transactions\nMIN(TranAmt) OVER (PARTITION BY AccountId ORDER BY TranDate ROWS BETWEEN 2 PRECEDING AND CURRENT ROW) as SlideMin,\n-- largest of the current and previous 2 transactions\nMAX(TranAmt) OVER (PARTITION BY AccountId ORDER BY TranDate ROWS BETWEEN 2 PRECEDING AND CURRENT ROW) as SlideMax,\n-- total of the current and previous 2 transactions\nSUM(TranAmt) OVER (PARTITION BY AccountId ORDER BY TranDate ROWS BETWEEN 2 PRECEDING AND CURRENT ROW) as SlideTotal,\nROW_NUMBER() OVER (PARTITION BY AccountId ORDER BY TranDate) AS RN\nFROM Sample.Transactions \nORDER BY AccountId, TranDate, RN"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cc211820-c278-4602-b42d-9f3099c95e7e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[[1,"2011-01-01","500.00","500.000000",1,"500.00","500.00","500.00",1],[1,"2011-01-15","50.00","275.000000",2,"50.00","500.00","550.00",2],[1,"2011-01-22","250.00","266.666667",3,"50.00","500.00","800.00",3],[1,"2011-01-24","75.00","125.000000",3,"50.00","250.00","375.00",4],[1,"2011-01-26","125.00","150.000000",3,"75.00","250.00","450.00",5],[1,"2011-01-28","175.00","125.000000",3,"75.00","175.00","375.00",6],[2,"2011-01-01","500.00","500.000000",1,"500.00","500.00","500.00",1],[2,"2011-01-15","50.00","275.000000",2,"50.00","500.00","550.00",2],[2,"2011-01-22","25.00","191.666667",3,"25.00","500.00","575.00",3],[2,"2011-01-23","125.00","66.666667",3,"25.00","125.00","200.00",4],[2,"2011-01-26","200.00","116.666667",3,"25.00","200.00","350.00",5],[2,"2011-01-29","250.00","191.666667",3,"125.00","250.00","575.00",6],[3,"2011-01-01","500.00","500.000000",1,"500.00","500.00","500.00",1],[3,"2011-01-15","50.00","275.000000",2,"50.00","500.00","550.00",2],[3,"2011-01-22","5000.00","1850.000000",3,"50.00","5000.00","5550.00",3],[3,"2011-01-25","550.00","1866.666667",3,"50.00","5000.00","5600.00",4],[3,"2011-01-27","95.00","1881.666667",3,"95.00","5000.00","5645.00",5],[3,"2011-01-30","2500.00","1048.333333",3,"95.00","2500.00","3145.00",6]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"AccountId","type":"\"integer\"","metadata":"{}"},{"name":"TranDate","type":"\"date\"","metadata":"{}"},{"name":"TranAmt","type":"\"decimal(8,2)\"","metadata":"{}"},{"name":"SlideAvg","type":"\"decimal(12,6)\"","metadata":"{}"},{"name":"SlideQty","type":"\"long\"","metadata":"{}"},{"name":"SlideMin","type":"\"decimal(8,2)\"","metadata":"{}"},{"name":"SlideMax","type":"\"decimal(8,2)\"","metadata":"{}"},{"name":"SlideTotal","type":"\"decimal(18,2)\"","metadata":"{}"},{"name":"RN","type":"\"integer\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>AccountId</th><th>TranDate</th><th>TranAmt</th><th>SlideAvg</th><th>SlideQty</th><th>SlideMin</th><th>SlideMax</th><th>SlideTotal</th><th>RN</th></tr></thead><tbody><tr><td>1</td><td>2011-01-01</td><td>500.00</td><td>500.000000</td><td>1</td><td>500.00</td><td>500.00</td><td>500.00</td><td>1</td></tr><tr><td>1</td><td>2011-01-15</td><td>50.00</td><td>275.000000</td><td>2</td><td>50.00</td><td>500.00</td><td>550.00</td><td>2</td></tr><tr><td>1</td><td>2011-01-22</td><td>250.00</td><td>266.666667</td><td>3</td><td>50.00</td><td>500.00</td><td>800.00</td><td>3</td></tr><tr><td>1</td><td>2011-01-24</td><td>75.00</td><td>125.000000</td><td>3</td><td>50.00</td><td>250.00</td><td>375.00</td><td>4</td></tr><tr><td>1</td><td>2011-01-26</td><td>125.00</td><td>150.000000</td><td>3</td><td>75.00</td><td>250.00</td><td>450.00</td><td>5</td></tr><tr><td>1</td><td>2011-01-28</td><td>175.00</td><td>125.000000</td><td>3</td><td>75.00</td><td>175.00</td><td>375.00</td><td>6</td></tr><tr><td>2</td><td>2011-01-01</td><td>500.00</td><td>500.000000</td><td>1</td><td>500.00</td><td>500.00</td><td>500.00</td><td>1</td></tr><tr><td>2</td><td>2011-01-15</td><td>50.00</td><td>275.000000</td><td>2</td><td>50.00</td><td>500.00</td><td>550.00</td><td>2</td></tr><tr><td>2</td><td>2011-01-22</td><td>25.00</td><td>191.666667</td><td>3</td><td>25.00</td><td>500.00</td><td>575.00</td><td>3</td></tr><tr><td>2</td><td>2011-01-23</td><td>125.00</td><td>66.666667</td><td>3</td><td>25.00</td><td>125.00</td><td>200.00</td><td>4</td></tr><tr><td>2</td><td>2011-01-26</td><td>200.00</td><td>116.666667</td><td>3</td><td>25.00</td><td>200.00</td><td>350.00</td><td>5</td></tr><tr><td>2</td><td>2011-01-29</td><td>250.00</td><td>191.666667</td><td>3</td><td>125.00</td><td>250.00</td><td>575.00</td><td>6</td></tr><tr><td>3</td><td>2011-01-01</td><td>500.00</td><td>500.000000</td><td>1</td><td>500.00</td><td>500.00</td><td>500.00</td><td>1</td></tr><tr><td>3</td><td>2011-01-15</td><td>50.00</td><td>275.000000</td><td>2</td><td>50.00</td><td>500.00</td><td>550.00</td><td>2</td></tr><tr><td>3</td><td>2011-01-22</td><td>5000.00</td><td>1850.000000</td><td>3</td><td>50.00</td><td>5000.00</td><td>5550.00</td><td>3</td></tr><tr><td>3</td><td>2011-01-25</td><td>550.00</td><td>1866.666667</td><td>3</td><td>50.00</td><td>5000.00</td><td>5600.00</td><td>4</td></tr><tr><td>3</td><td>2011-01-27</td><td>95.00</td><td>1881.666667</td><td>3</td><td>95.00</td><td>5000.00</td><td>5645.00</td><td>5</td></tr><tr><td>3</td><td>2011-01-30</td><td>2500.00</td><td>1048.333333</td><td>3</td><td>95.00</td><td>2500.00</td><td>3145.00</td><td>6</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import row_number\n\nwindowSpec = Window.partitionBy(\"AccountId\").orderBy(\"TranDate\").rowsBetween(-2,Window.currentRow)\nwindowSpec2 = Window.partitionBy(\"AccountId\").orderBy(\"TranDate\")\nsumTransactionsDf = data_transactions \\\n  .withColumn(\"SlideAvg\",avg(\"TranAmt\").over(windowSpec)) \\\n  .withColumn(\"SlideQty\",count(\"*\").over(windowSpec)) \\\n  .withColumn(\"SlideMin\",min(\"TranAmt\").over(windowSpec)) \\\n  .withColumn(\"SlideMax\",max(\"TranAmt\").over(windowSpec)) \\\n  .withColumn(\"SlideTotal\",sum(\"TranAmt\").over(windowSpec)) \\\n  .withColumn(\"RN\", row_number().over(windowSpec2)) \\\n  .orderBy(\"AccountId\", \"TranDate\")\ndisplay(sumTransactionsDf)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a3e496e2-c642-41e6-8ee1-2e64f4fde028"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[[1,"2011-01-01",500,500.0,1,500,500,500,1],[1,"2011-01-15",50,275.0,2,50,500,550,2],[1,"2011-01-22",250,266.6666666666667,3,50,500,800,3],[1,"2011-01-24",75,125.0,3,50,250,375,4],[1,"2011-01-26",125,150.0,3,75,250,450,5],[1,"2011-01-28",175,125.0,3,75,175,375,6],[2,"2011-01-01",500,500.0,1,500,500,500,1],[2,"2011-01-15",50,275.0,2,50,500,550,2],[2,"2011-01-22",25,191.66666666666666,3,25,500,575,3],[2,"2011-01-23",125,66.66666666666667,3,25,125,200,4],[2,"2011-01-26",200,116.66666666666667,3,25,200,350,5],[2,"2011-01-29",250,191.66666666666666,3,125,250,575,6],[3,"2011-01-01",500,500.0,1,500,500,500,1],[3,"2011-01-15",50,275.0,2,50,500,550,2],[3,"2011-01-22",5000,1850.0,3,50,5000,5550,3],[3,"2011-01-25",550,1866.6666666666667,3,50,5000,5600,4],[3,"2011-01-27",95,1881.6666666666667,3,95,5000,5645,5],[3,"2011-01-30",2500,1048.3333333333333,3,95,2500,3145,6]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"AccountId","type":"\"long\"","metadata":"{}"},{"name":"TranDate","type":"\"date\"","metadata":"{}"},{"name":"TranAmt","type":"\"long\"","metadata":"{}"},{"name":"SlideAvg","type":"\"double\"","metadata":"{}"},{"name":"SlideQty","type":"\"long\"","metadata":"{}"},{"name":"SlideMin","type":"\"long\"","metadata":"{}"},{"name":"SlideMax","type":"\"long\"","metadata":"{}"},{"name":"SlideTotal","type":"\"long\"","metadata":"{}"},{"name":"RN","type":"\"integer\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>AccountId</th><th>TranDate</th><th>TranAmt</th><th>SlideAvg</th><th>SlideQty</th><th>SlideMin</th><th>SlideMax</th><th>SlideTotal</th><th>RN</th></tr></thead><tbody><tr><td>1</td><td>2011-01-01</td><td>500</td><td>500.0</td><td>1</td><td>500</td><td>500</td><td>500</td><td>1</td></tr><tr><td>1</td><td>2011-01-15</td><td>50</td><td>275.0</td><td>2</td><td>50</td><td>500</td><td>550</td><td>2</td></tr><tr><td>1</td><td>2011-01-22</td><td>250</td><td>266.6666666666667</td><td>3</td><td>50</td><td>500</td><td>800</td><td>3</td></tr><tr><td>1</td><td>2011-01-24</td><td>75</td><td>125.0</td><td>3</td><td>50</td><td>250</td><td>375</td><td>4</td></tr><tr><td>1</td><td>2011-01-26</td><td>125</td><td>150.0</td><td>3</td><td>75</td><td>250</td><td>450</td><td>5</td></tr><tr><td>1</td><td>2011-01-28</td><td>175</td><td>125.0</td><td>3</td><td>75</td><td>175</td><td>375</td><td>6</td></tr><tr><td>2</td><td>2011-01-01</td><td>500</td><td>500.0</td><td>1</td><td>500</td><td>500</td><td>500</td><td>1</td></tr><tr><td>2</td><td>2011-01-15</td><td>50</td><td>275.0</td><td>2</td><td>50</td><td>500</td><td>550</td><td>2</td></tr><tr><td>2</td><td>2011-01-22</td><td>25</td><td>191.66666666666666</td><td>3</td><td>25</td><td>500</td><td>575</td><td>3</td></tr><tr><td>2</td><td>2011-01-23</td><td>125</td><td>66.66666666666667</td><td>3</td><td>25</td><td>125</td><td>200</td><td>4</td></tr><tr><td>2</td><td>2011-01-26</td><td>200</td><td>116.66666666666667</td><td>3</td><td>25</td><td>200</td><td>350</td><td>5</td></tr><tr><td>2</td><td>2011-01-29</td><td>250</td><td>191.66666666666666</td><td>3</td><td>125</td><td>250</td><td>575</td><td>6</td></tr><tr><td>3</td><td>2011-01-01</td><td>500</td><td>500.0</td><td>1</td><td>500</td><td>500</td><td>500</td><td>1</td></tr><tr><td>3</td><td>2011-01-15</td><td>50</td><td>275.0</td><td>2</td><td>50</td><td>500</td><td>550</td><td>2</td></tr><tr><td>3</td><td>2011-01-22</td><td>5000</td><td>1850.0</td><td>3</td><td>50</td><td>5000</td><td>5550</td><td>3</td></tr><tr><td>3</td><td>2011-01-25</td><td>550</td><td>1866.6666666666667</td><td>3</td><td>50</td><td>5000</td><td>5600</td><td>4</td></tr><tr><td>3</td><td>2011-01-27</td><td>95</td><td>1881.6666666666667</td><td>3</td><td>95</td><td>5000</td><td>5645</td><td>5</td></tr><tr><td>3</td><td>2011-01-30</td><td>2500</td><td>1048.3333333333333</td><td>3</td><td>95</td><td>2500</td><td>3145</td><td>6</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["%sql\nSELECT RowID,\nFName,\nSalary,\nSUM(Salary) OVER (ORDER BY Salary ROWS UNBOUNDED PRECEDING) as SumByRows,\nSUM(Salary) OVER (ORDER BY Salary RANGE UNBOUNDED PRECEDING) as SumByRange\n\nFROM Sample.Logical\nORDER BY RowID"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5a1f7913-c3cd-433c-a58f-0674f3686c2e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[[1,"George",800,800,800],[2,"Sam",950,1750,1750],[3,"Diane",1100,2850,2850],[4,"Nicholas",1250,4100,5350],[5,"Samuel",1250,5350,5350],[6,"Patricia",1300,6650,6650],[7,"Brian",1500,8150,8150],[8,"Thomas",1600,9750,9750],[9,"Fran",2450,12200,12200],[10,"Debbie",2850,15050,15050],[11,"Mark",2975,18025,18025],[12,"James",3000,24025,24025],[13,"Cynthia",3000,21025,24025],[14,"Christopher",5000,29025,29025]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"RowID","type":"\"integer\"","metadata":"{}"},{"name":"FName","type":"\"string\"","metadata":"{\"__CHAR_VARCHAR_TYPE_STRING\":\"varchar(20)\"}"},{"name":"Salary","type":"\"short\"","metadata":"{}"},{"name":"SumByRows","type":"\"long\"","metadata":"{}"},{"name":"SumByRange","type":"\"long\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>RowID</th><th>FName</th><th>Salary</th><th>SumByRows</th><th>SumByRange</th></tr></thead><tbody><tr><td>1</td><td>George</td><td>800</td><td>800</td><td>800</td></tr><tr><td>2</td><td>Sam</td><td>950</td><td>1750</td><td>1750</td></tr><tr><td>3</td><td>Diane</td><td>1100</td><td>2850</td><td>2850</td></tr><tr><td>4</td><td>Nicholas</td><td>1250</td><td>4100</td><td>5350</td></tr><tr><td>5</td><td>Samuel</td><td>1250</td><td>5350</td><td>5350</td></tr><tr><td>6</td><td>Patricia</td><td>1300</td><td>6650</td><td>6650</td></tr><tr><td>7</td><td>Brian</td><td>1500</td><td>8150</td><td>8150</td></tr><tr><td>8</td><td>Thomas</td><td>1600</td><td>9750</td><td>9750</td></tr><tr><td>9</td><td>Fran</td><td>2450</td><td>12200</td><td>12200</td></tr><tr><td>10</td><td>Debbie</td><td>2850</td><td>15050</td><td>15050</td></tr><tr><td>11</td><td>Mark</td><td>2975</td><td>18025</td><td>18025</td></tr><tr><td>12</td><td>James</td><td>3000</td><td>24025</td><td>24025</td></tr><tr><td>13</td><td>Cynthia</td><td>3000</td><td>21025</td><td>24025</td></tr><tr><td>14</td><td>Christopher</td><td>5000</td><td>29025</td><td>29025</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["\nwindowRows = Window.orderBy(\"Salary\").rowsBetween(Window.unboundedPreceding, Window.currentRow)\nwindowRange = Window.orderBy(\"Salary\").rangeBetween(Window.unboundedPreceding, Window.currentRow)\nlogicalDf = data_logical \\\n  .withColumn(\"SumByRows\",sum(\"Salary\").over(windowRows)) \\\n  .withColumn(\"SumByRange\",sum(\"Salary\").over(windowRange)) \\\n  .orderBy(\"RowID\")\ndisplay(logicalDf)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fbed3f16-69ba-4691-8c49-e6623491ee50"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[[1,"George",800,800,800],[2,"Sam",950,1750,1750],[3,"Diane",1100,2850,2850],[4,"Nicholas",1250,4100,5350],[5,"Samuel",1250,5350,5350],[6,"Patricia",1300,6650,6650],[7,"Brian",1500,8150,8150],[8,"Thomas",1600,9750,9750],[9,"Fran",2450,12200,12200],[10,"Debbie",2850,15050,15050],[11,"Mark",2975,18025,18025],[12,"James",3000,21025,24025],[13,"Cynthia",3000,24025,24025],[14,"Christopher",5000,29025,29025]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"RowID","type":"\"long\"","metadata":"{}"},{"name":"FName","type":"\"string\"","metadata":"{}"},{"name":"Salary","type":"\"long\"","metadata":"{}"},{"name":"SumByRows","type":"\"long\"","metadata":"{}"},{"name":"SumByRange","type":"\"long\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>RowID</th><th>FName</th><th>Salary</th><th>SumByRows</th><th>SumByRange</th></tr></thead><tbody><tr><td>1</td><td>George</td><td>800</td><td>800</td><td>800</td></tr><tr><td>2</td><td>Sam</td><td>950</td><td>1750</td><td>1750</td></tr><tr><td>3</td><td>Diane</td><td>1100</td><td>2850</td><td>2850</td></tr><tr><td>4</td><td>Nicholas</td><td>1250</td><td>4100</td><td>5350</td></tr><tr><td>5</td><td>Samuel</td><td>1250</td><td>5350</td><td>5350</td></tr><tr><td>6</td><td>Patricia</td><td>1300</td><td>6650</td><td>6650</td></tr><tr><td>7</td><td>Brian</td><td>1500</td><td>8150</td><td>8150</td></tr><tr><td>8</td><td>Thomas</td><td>1600</td><td>9750</td><td>9750</td></tr><tr><td>9</td><td>Fran</td><td>2450</td><td>12200</td><td>12200</td></tr><tr><td>10</td><td>Debbie</td><td>2850</td><td>15050</td><td>15050</td></tr><tr><td>11</td><td>Mark</td><td>2975</td><td>18025</td><td>18025</td></tr><tr><td>12</td><td>James</td><td>3000</td><td>21025</td><td>24025</td></tr><tr><td>13</td><td>Cynthia</td><td>3000</td><td>24025</td><td>24025</td></tr><tr><td>14</td><td>Christopher</td><td>5000</td><td>29025</td><td>29025</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["%sql\nSELECT \nAccountId,\nTranDate,\nTranAmt,\nROW_NUMBER() OVER (PARTITION BY TranAmt ORDER BY TranDate) AS RN\nFROM Sample.Transactions\nORDER BY TranDate, RN\nLIMIT 10"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e6ea07d3-2495-4dbb-98c0-7c1a77e92330"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[[2,"2011-01-01","500.00",1],[1,"2011-01-01","500.00",2],[3,"2011-01-01","500.00",3],[2,"2011-01-15","50.00",1],[1,"2011-01-15","50.00",2],[3,"2011-01-15","50.00",3],[2,"2011-01-22","25.00",1],[3,"2011-01-22","5000.00",1],[1,"2011-01-22","250.00",1],[2,"2011-01-23","125.00",1]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"AccountId","type":"\"integer\"","metadata":"{}"},{"name":"TranDate","type":"\"date\"","metadata":"{}"},{"name":"TranAmt","type":"\"decimal(8,2)\"","metadata":"{}"},{"name":"RN","type":"\"integer\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>AccountId</th><th>TranDate</th><th>TranAmt</th><th>RN</th></tr></thead><tbody><tr><td>2</td><td>2011-01-01</td><td>500.00</td><td>1</td></tr><tr><td>1</td><td>2011-01-01</td><td>500.00</td><td>2</td></tr><tr><td>3</td><td>2011-01-01</td><td>500.00</td><td>3</td></tr><tr><td>2</td><td>2011-01-15</td><td>50.00</td><td>1</td></tr><tr><td>1</td><td>2011-01-15</td><td>50.00</td><td>2</td></tr><tr><td>3</td><td>2011-01-15</td><td>50.00</td><td>3</td></tr><tr><td>2</td><td>2011-01-22</td><td>25.00</td><td>1</td></tr><tr><td>3</td><td>2011-01-22</td><td>5000.00</td><td>1</td></tr><tr><td>1</td><td>2011-01-22</td><td>250.00</td><td>1</td></tr><tr><td>2</td><td>2011-01-23</td><td>125.00</td><td>1</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["windowSpec = Window.partitionBy(\"TranAmt\").orderBy(\"TranDate\")\ntransactionsDf = data_transactions \\\n  .withColumn(\"RN\",row_number().over(windowSpec)) \\\n  .orderBy(\"TranDate\", \"RN\") \\\n  .head(10)\ndisplay(transactionsDf)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"89611ee1-f8b4-4732-811c-3ef08076c357"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[[1,"2011-01-01",500,1],[2,"2011-01-01",500,2],[3,"2011-01-01",500,3],[1,"2011-01-15",50,1],[2,"2011-01-15",50,2],[3,"2011-01-15",50,3],[2,"2011-01-22",25,1],[3,"2011-01-22",5000,1],[1,"2011-01-22",250,1],[2,"2011-01-23",125,1]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"AccountId","type":"\"long\"","metadata":"{}"},{"name":"TranDate","type":"\"date\"","metadata":"{}"},{"name":"TranAmt","type":"\"long\"","metadata":"{}"},{"name":"RN","type":"\"long\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>AccountId</th><th>TranDate</th><th>TranAmt</th><th>RN</th></tr></thead><tbody><tr><td>1</td><td>2011-01-01</td><td>500</td><td>1</td></tr><tr><td>2</td><td>2011-01-01</td><td>500</td><td>2</td></tr><tr><td>3</td><td>2011-01-01</td><td>500</td><td>3</td></tr><tr><td>1</td><td>2011-01-15</td><td>50</td><td>1</td></tr><tr><td>2</td><td>2011-01-15</td><td>50</td><td>2</td></tr><tr><td>3</td><td>2011-01-15</td><td>50</td><td>3</td></tr><tr><td>2</td><td>2011-01-22</td><td>25</td><td>1</td></tr><tr><td>3</td><td>2011-01-22</td><td>5000</td><td>1</td></tr><tr><td>1</td><td>2011-01-22</td><td>250</td><td>1</td></tr><tr><td>2</td><td>2011-01-23</td><td>125</td><td>1</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import lead,lag,first,last,row_number,dense_rank\n# too lazy to write the whole name every time\ndt = data_transactions\n\nw = Window.partitionBy('TranAmt').orderBy('TranDate')\n\ndisplay(dt.select(first('AccountId').over(w), lead('AccountId').over(w), lag('AccountId').over(w), last('AccountId').over(w), row_number().over(w), dense_rank().over(w)))\n\n\n# I don't understad what \"Kad z funkcji wykonaj dla ROWS i RANGE i BETWEEN\" means"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fd220afe-2a81-4a7c-a3ff-87ba12f3d301"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[[2,null,null,2,1,1],[1,2,null,3,1,1],[1,3,1,3,2,1],[1,null,2,3,3,1],[1,null,null,1,1,1],[3,null,null,3,1,1],[2,1,null,2,1,1],[2,null,2,1,2,2],[1,null,null,1,1,1],[2,null,null,2,1,1],[1,2,null,1,1,1],[1,null,1,2,2,2],[1,2,null,3,1,1],[1,3,1,3,2,1],[1,null,2,3,3,1],[3,null,null,3,1,1],[3,null,null,3,1,1],[3,null,null,3,1,1]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"first(AccountId) OVER (PARTITION BY TranAmt ORDER BY TranDate ASC NULLS FIRST RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)","type":"\"long\"","metadata":"{}"},{"name":"lead(AccountId, 1, NULL) OVER (PARTITION BY TranAmt ORDER BY TranDate ASC NULLS FIRST ROWS BETWEEN 1 FOLLOWING AND 1 FOLLOWING)","type":"\"long\"","metadata":"{}"},{"name":"lag(AccountId, 1, NULL) OVER (PARTITION BY TranAmt ORDER BY TranDate ASC NULLS FIRST ROWS BETWEEN -1 FOLLOWING AND -1 FOLLOWING)","type":"\"long\"","metadata":"{}"},{"name":"last(AccountId) OVER (PARTITION BY TranAmt ORDER BY TranDate ASC NULLS FIRST RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)","type":"\"long\"","metadata":"{}"},{"name":"row_number() OVER (PARTITION BY TranAmt ORDER BY TranDate ASC NULLS FIRST ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)","type":"\"integer\"","metadata":"{}"},{"name":"DENSE_RANK() OVER (PARTITION BY TranAmt ORDER BY TranDate ASC NULLS FIRST ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)","type":"\"integer\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>first(AccountId) OVER (PARTITION BY TranAmt ORDER BY TranDate ASC NULLS FIRST RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)</th><th>lead(AccountId, 1, NULL) OVER (PARTITION BY TranAmt ORDER BY TranDate ASC NULLS FIRST ROWS BETWEEN 1 FOLLOWING AND 1 FOLLOWING)</th><th>lag(AccountId, 1, NULL) OVER (PARTITION BY TranAmt ORDER BY TranDate ASC NULLS FIRST ROWS BETWEEN -1 FOLLOWING AND -1 FOLLOWING)</th><th>last(AccountId) OVER (PARTITION BY TranAmt ORDER BY TranDate ASC NULLS FIRST RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)</th><th>row_number() OVER (PARTITION BY TranAmt ORDER BY TranDate ASC NULLS FIRST ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)</th><th>DENSE_RANK() OVER (PARTITION BY TranAmt ORDER BY TranDate ASC NULLS FIRST ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)</th></tr></thead><tbody><tr><td>2</td><td>null</td><td>null</td><td>2</td><td>1</td><td>1</td></tr><tr><td>1</td><td>2</td><td>null</td><td>3</td><td>1</td><td>1</td></tr><tr><td>1</td><td>3</td><td>1</td><td>3</td><td>2</td><td>1</td></tr><tr><td>1</td><td>null</td><td>2</td><td>3</td><td>3</td><td>1</td></tr><tr><td>1</td><td>null</td><td>null</td><td>1</td><td>1</td><td>1</td></tr><tr><td>3</td><td>null</td><td>null</td><td>3</td><td>1</td><td>1</td></tr><tr><td>2</td><td>1</td><td>null</td><td>2</td><td>1</td><td>1</td></tr><tr><td>2</td><td>null</td><td>2</td><td>1</td><td>2</td><td>2</td></tr><tr><td>1</td><td>null</td><td>null</td><td>1</td><td>1</td><td>1</td></tr><tr><td>2</td><td>null</td><td>null</td><td>2</td><td>1</td><td>1</td></tr><tr><td>1</td><td>2</td><td>null</td><td>1</td><td>1</td><td>1</td></tr><tr><td>1</td><td>null</td><td>1</td><td>2</td><td>2</td><td>2</td></tr><tr><td>1</td><td>2</td><td>null</td><td>3</td><td>1</td><td>1</td></tr><tr><td>1</td><td>3</td><td>1</td><td>3</td><td>2</td><td>1</td></tr><tr><td>1</td><td>null</td><td>2</td><td>3</td><td>3</td><td>1</td></tr><tr><td>3</td><td>null</td><td>null</td><td>3</td><td>1</td><td>1</td></tr><tr><td>3</td><td>null</td><td>null</td><td>3</td><td>1</td><td>1</td></tr><tr><td>3</td><td>null</td><td>null</td><td>3</td><td>1</td><td>1</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["dt.join(data_logical, dt.AccountId == data_logical.RowID, \"leftanti\").explain()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cb9754ea-ca8a-4cc0-9175-8966cdb44ad8"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- Project [AccountId#545L, cast(TranDate#546 as date) AS TranDate#551, TranAmt#547L]\n   +- SortMergeJoin [AccountId#545L], [RowID#515L], LeftAnti\n      :- Sort [AccountId#545L ASC NULLS FIRST], false, 0\n      :  +- Exchange hashpartitioning(AccountId#545L, 200), ENSURE_REQUIREMENTS, [id=#1606]\n      :     +- Scan ExistingRDD[AccountId#545L,TranDate#546,TranAmt#547L]\n      +- Sort [RowID#515L ASC NULLS FIRST], false, 0\n         +- Exchange hashpartitioning(RowID#515L, 200), ENSURE_REQUIREMENTS, [id=#1607]\n            +- Project [RowID#515L]\n               +- Filter isnotnull(RowID#515L)\n                  +- Scan ExistingRDD[RowID#515L,FName#516,Salary#517L]\n\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- Project [AccountId#545L, cast(TranDate#546 as date) AS TranDate#551, TranAmt#547L]\n   +- SortMergeJoin [AccountId#545L], [RowID#515L], LeftAnti\n      :- Sort [AccountId#545L ASC NULLS FIRST], false, 0\n      :  +- Exchange hashpartitioning(AccountId#545L, 200), ENSURE_REQUIREMENTS, [id=#1606]\n      :     +- Scan ExistingRDD[AccountId#545L,TranDate#546,TranAmt#547L]\n      +- Sort [RowID#515L ASC NULLS FIRST], false, 0\n         +- Exchange hashpartitioning(RowID#515L, 200), ENSURE_REQUIREMENTS, [id=#1607]\n            +- Project [RowID#515L]\n               +- Filter isnotnull(RowID#515L)\n                  +- Scan ExistingRDD[RowID#515L,FName#516,Salary#517L]\n\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["dt.join(data_logical, dt.AccountId == data_logical.RowID, \"leftsemi\").explain()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"71a14c36-63f8-433e-8a95-0598ef1e6985"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- Project [AccountId#545L, cast(TranDate#546 as date) AS TranDate#551, TranAmt#547L]\n   +- SortMergeJoin [AccountId#545L], [RowID#515L], LeftSemi\n      :- Sort [AccountId#545L ASC NULLS FIRST], false, 0\n      :  +- Exchange hashpartitioning(AccountId#545L, 200), ENSURE_REQUIREMENTS, [id=#1685]\n      :     +- Filter isnotnull(AccountId#545L)\n      :        +- Scan ExistingRDD[AccountId#545L,TranDate#546,TranAmt#547L]\n      +- Sort [RowID#515L ASC NULLS FIRST], false, 0\n         +- Exchange hashpartitioning(RowID#515L, 200), ENSURE_REQUIREMENTS, [id=#1686]\n            +- Project [RowID#515L]\n               +- Filter isnotnull(RowID#515L)\n                  +- Scan ExistingRDD[RowID#515L,FName#516,Salary#517L]\n\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- Project [AccountId#545L, cast(TranDate#546 as date) AS TranDate#551, TranAmt#547L]\n   +- SortMergeJoin [AccountId#545L], [RowID#515L], LeftSemi\n      :- Sort [AccountId#545L ASC NULLS FIRST], false, 0\n      :  +- Exchange hashpartitioning(AccountId#545L, 200), ENSURE_REQUIREMENTS, [id=#1685]\n      :     +- Filter isnotnull(AccountId#545L)\n      :        +- Scan ExistingRDD[AccountId#545L,TranDate#546,TranAmt#547L]\n      +- Sort [RowID#515L ASC NULLS FIRST], false, 0\n         +- Exchange hashpartitioning(RowID#515L, 200), ENSURE_REQUIREMENTS, [id=#1686]\n            +- Project [RowID#515L]\n               +- Filter isnotnull(RowID#515L)\n                  +- Scan ExistingRDD[RowID#515L,FName#516,Salary#517L]\n\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["dt.join(data_logical, dt.AccountId == data_logical.RowID).drop(col('RowID')).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5070c619-3e2f-4284-9e88-c9958100addc"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+---------+----------+-------+------+------+\n|AccountId|  TranDate|TranAmt| FName|Salary|\n+---------+----------+-------+------+------+\n|        1|2011-01-01|    500|George|   800|\n|        1|2011-01-15|     50|George|   800|\n|        1|2011-01-22|    250|George|   800|\n|        1|2011-01-24|     75|George|   800|\n|        1|2011-01-26|    125|George|   800|\n|        1|2011-01-28|    175|George|   800|\n|        2|2011-01-01|    500|   Sam|   950|\n|        2|2011-01-15|     50|   Sam|   950|\n|        2|2011-01-22|     25|   Sam|   950|\n|        2|2011-01-23|    125|   Sam|   950|\n|        2|2011-01-26|    200|   Sam|   950|\n|        2|2011-01-29|    250|   Sam|   950|\n|        3|2011-01-01|    500| Diane|  1100|\n|        3|2011-01-15|     50| Diane|  1100|\n|        3|2011-01-22|   5000| Diane|  1100|\n|        3|2011-01-25|    550| Diane|  1100|\n|        3|2011-01-27|     95| Diane|  1100|\n|        3|2011-01-30|   2500| Diane|  1100|\n+---------+----------+-------+------+------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+---------+----------+-------+------+------+\n|AccountId|  TranDate|TranAmt| FName|Salary|\n+---------+----------+-------+------+------+\n|        1|2011-01-01|    500|George|   800|\n|        1|2011-01-15|     50|George|   800|\n|        1|2011-01-22|    250|George|   800|\n|        1|2011-01-24|     75|George|   800|\n|        1|2011-01-26|    125|George|   800|\n|        1|2011-01-28|    175|George|   800|\n|        2|2011-01-01|    500|   Sam|   950|\n|        2|2011-01-15|     50|   Sam|   950|\n|        2|2011-01-22|     25|   Sam|   950|\n|        2|2011-01-23|    125|   Sam|   950|\n|        2|2011-01-26|    200|   Sam|   950|\n|        2|2011-01-29|    250|   Sam|   950|\n|        3|2011-01-01|    500| Diane|  1100|\n|        3|2011-01-15|     50| Diane|  1100|\n|        3|2011-01-22|   5000| Diane|  1100|\n|        3|2011-01-25|    550| Diane|  1100|\n|        3|2011-01-27|     95| Diane|  1100|\n|        3|2011-01-30|   2500| Diane|  1100|\n+---------+----------+-------+------+------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["dt.join(data_logical, dt.AccountId == data_logical.RowID).select(col('AccountId'), col('TranDate'), col('TranAmt'), col('FName'),col('Salary') ).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"041bdcbf-4479-476b-9cf3-7bebad2f5d45"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+---------+----------+-------+------+------+\n|AccountId|  TranDate|TranAmt| FName|Salary|\n+---------+----------+-------+------+------+\n|        1|2011-01-01|    500|George|   800|\n|        1|2011-01-15|     50|George|   800|\n|        1|2011-01-22|    250|George|   800|\n|        1|2011-01-24|     75|George|   800|\n|        1|2011-01-26|    125|George|   800|\n|        1|2011-01-28|    175|George|   800|\n|        2|2011-01-01|    500|   Sam|   950|\n|        2|2011-01-15|     50|   Sam|   950|\n|        2|2011-01-22|     25|   Sam|   950|\n|        2|2011-01-23|    125|   Sam|   950|\n|        2|2011-01-26|    200|   Sam|   950|\n|        2|2011-01-29|    250|   Sam|   950|\n|        3|2011-01-01|    500| Diane|  1100|\n|        3|2011-01-15|     50| Diane|  1100|\n|        3|2011-01-22|   5000| Diane|  1100|\n|        3|2011-01-25|    550| Diane|  1100|\n|        3|2011-01-27|     95| Diane|  1100|\n|        3|2011-01-30|   2500| Diane|  1100|\n+---------+----------+-------+------+------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+---------+----------+-------+------+------+\n|AccountId|  TranDate|TranAmt| FName|Salary|\n+---------+----------+-------+------+------+\n|        1|2011-01-01|    500|George|   800|\n|        1|2011-01-15|     50|George|   800|\n|        1|2011-01-22|    250|George|   800|\n|        1|2011-01-24|     75|George|   800|\n|        1|2011-01-26|    125|George|   800|\n|        1|2011-01-28|    175|George|   800|\n|        2|2011-01-01|    500|   Sam|   950|\n|        2|2011-01-15|     50|   Sam|   950|\n|        2|2011-01-22|     25|   Sam|   950|\n|        2|2011-01-23|    125|   Sam|   950|\n|        2|2011-01-26|    200|   Sam|   950|\n|        2|2011-01-29|    250|   Sam|   950|\n|        3|2011-01-01|    500| Diane|  1100|\n|        3|2011-01-15|     50| Diane|  1100|\n|        3|2011-01-22|   5000| Diane|  1100|\n|        3|2011-01-25|    550| Diane|  1100|\n|        3|2011-01-27|     95| Diane|  1100|\n|        3|2011-01-30|   2500| Diane|  1100|\n+---------+----------+-------+------+------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import broadcast\n\ndt.join(broadcast(data_logical)).explain()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5b49b877-87be-4dfe-bb27-e40364e1b657"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- BroadcastNestedLoopJoin BuildRight, Inner\n   :- Project [AccountId#545L, cast(TranDate#546 as date) AS TranDate#551, TranAmt#547L]\n   :  +- Scan ExistingRDD[AccountId#545L,TranDate#546,TranAmt#547L]\n   +- BroadcastExchange IdentityBroadcastMode, [id=#2784]\n      +- Scan ExistingRDD[RowID#515L,FName#516,Salary#517L]\n\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- BroadcastNestedLoopJoin BuildRight, Inner\n   :- Project [AccountId#545L, cast(TranDate#546 as date) AS TranDate#551, TranAmt#547L]\n   :  +- Scan ExistingRDD[AccountId#545L,TranDate#546,TranAmt#547L]\n   +- BroadcastExchange IdentityBroadcastMode, [id=#2784]\n      +- Scan ExistingRDD[RowID#515L,FName#516,Salary#517L]\n\n\n"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"lab6","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":126973269320608}},"nbformat":4,"nbformat_minor":0}
